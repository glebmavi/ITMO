{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUuyLVDZ4s8r"
   },
   "source": [
    "## ЗАДАНИЕ ДЛЯ ПРОГРАММИРУЮЩИХ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwJ5Q6w2IFHA"
   },
   "source": [
    "**Ссылка**, на источник текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "64uxt-07IEec"
   },
   "outputs": [],
   "source": [
    "# DATA_URL = \"http://az.lib.ru/t/tolstoj_a_k/text_0180.shtml\" Оригинальный текст. Все вычеркнутые значения относятся к этому тексту\n",
    "DATA_URL = \"http://az.lib.ru/t/tolstoj_a_k/text_0170.shtml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwrBkeLnHuA1"
   },
   "source": [
    "Устанавливаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5Fr5swwYfz5",
    "outputId": "826a3a2b-3980-4176-b20a-7453524a8d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install -q tensorflow==2.12\n",
    "!pip install -q git+https://github.com/dvolchek/rnnmorph.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbxMKqhPH1Dk"
   },
   "source": [
    "Создаём объект морфологического анализатора `RNNMorph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "24zMUhvi99AV"
   },
   "outputs": [],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59q1L9p0H9K9"
   },
   "source": [
    "Скачиваем текст, по которому будет дано задание, с помощью `urllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0uW0fw_h-Pft"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "opener = urllib.request.URLopener({})\n",
    "resource = opener.open(DATA_URL)\n",
    "raw_text = resource.read().decode(resource.headers.get_content_charset()) #Текс с html тегами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "-hSPOjDo4sdh",
    "outputId": "546dcc46-040d-4c8d-adc4-53883a0e6232"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<html>\\r\\n<head>\\r\\n<title>Lib.ru/Классика: Толстой Алексей Константинович. Упырь</title>\\r\\n</head>\\r\\n\\r\\n<body>\\r\\n\\r\\n\\r\\n<center>\\r\\n\\r\\n<h2><a href=/t/tolstoj_a_k/>Толстой Алексей Константинович</a><br>\\r\\nУпырь</h2>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZiLHQNSITAt"
   },
   "source": [
    "Как видно, текст содержит html теги, от которых нужно избавиться. Выбрасываем из текста HTML-теги с помощью библиотеки Beatiful soap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "We4LkyUMPfuq"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(raw_text, features=\"html.parser\")\n",
    "\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "cleaned_text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "lOD8PJnG4rbl",
    "outputId": "9c6145af-9662-443c-bc5d-370a618e8df7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nLib.ru/Классика: Толстой Алексей Константинович. Упырь\\n\\n\\n\\nТолстой Алексей Константинович\\r\\nУпырь\\n\\n\\nLib.ru/Классика:\\n\\r\\n\\n\\n[Регистрация]\\n \\n\\r\\n\\r\\n\\r\\n[Найти] \\r\\n[Рейтинги]\\r\\n[Обсуждения]\\r\\n[Новинки]\\r\\n[Обзоры]\\r\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14fYYb5hIpnY"
   },
   "source": [
    "С помощью библиотеки [NLTK](https://nltk.org/) разбиваем текст на предложения и токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "hRNu7jPvN6G_",
    "outputId": "ec700d5c-7c27-4af3-db11-30b76294e07a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"A total of 1464 'sentences'\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence, language=\"russian\") for sentence in sent_tokenize(cleaned_text)]\n",
    "\"A total of %d 'sentences'\" % len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRU4KEBAIyYT"
   },
   "source": [
    "## Задание 1\n",
    "С помощью метода `str.isalpha` из стандартной библиотеки Python модифицируйте нижеследующий код так, чтобы в predictions остались только буквенные токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4U5HH2CDPVUM",
    "outputId": "5165b516-83f7-4b76-e8b6-054865e52c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 13s 567ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentences: 100%|██████████| 1464/1464 [00:00<00:00, 78771.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "predictions = [[pred.normal_form for pred in sent if pred.normal_form.isalnum()]\n",
    "               for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), \"sentences\") ]\n",
    "predictions[-11:0] #Сейчас видно, что токены типа \"точка\", \"запятая\" и тд пока присутствуют в предложениях. От них нужно избавиться\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHGDhxhNJtTz"
   },
   "source": [
    "Проверьте себя. Должны получиться следующие значения:\n",
    "\n",
    "*   Предложений: ~577~ 1464 (возможны расхождения в несколько предложений)\n",
    "*   Токенов: примерно ~8621~ 20881 (возможны расхождения в некоторое количество токенов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwK_qRbw6sac",
    "outputId": "98ec0a6a-b95b-4f2d-9a08-20e50f3a0124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5jL4sWyKUnO",
    "outputId": "c7195c42-cfec-4189-df64-0a33baddc1a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20881"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_uniq_tokens = [word for sentence in predictions for word in sentence]\n",
    "len(non_uniq_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg2e-1hAKiT3"
   },
   "source": [
    "Для продолжения работы над заданием числа должны быть близки к указанным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mci9Nd5hKuJP"
   },
   "source": [
    "## Задание 2\n",
    "\n",
    "Используя `non_uniq_tokens`, стоп-слова для русского языка из библиотеки nltk (`nltk.corpus.stopwords`) и `nltk.FreqDist`, вычислите, **какую долю среди ~~100~~ 40 самых частотных** токенов в произведении занимают токены, **не относящиеся** к стоп словам.\n",
    "\n",
    "**Например**, если среди 100 самых частотных слов встречается 25 слов, входящих в стоп лист, значит не входят в стоп лист 75 слов, и их доля составит 0.75.\n",
    "\n",
    "**Не бойтесь использовать документацию NLTK и тьюториалы.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHbtLqkLKfZC",
    "outputId": "c71771e9-515b-4bf1-c8d4-e58a79b4f754"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = set(stopwords.words(\"russian\"))\n",
    "stopwords.words(\"russian\")[:5] #Пример стоп слов\n",
    "\n",
    "freq_dist = FreqDist(non_uniq_tokens)\n",
    "\n",
    "top_40_tokens = [token for token, _ in freq_dist.most_common(40)]\n",
    "\n",
    "non_stop_tokens = [token for token in top_40_tokens if token not in STOPWORDS]\n",
    "\n",
    "non_stop_ratio = len(non_stop_tokens) / 100\n",
    "non_stop_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezdbB95YwtSl"
   },
   "source": [
    "Проверьте себя: должно получиться ~0.49~ 0.19 (допустимо небольшое расхождение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HChyAdk2Ovx1"
   },
   "source": [
    "## Задание 3\n",
    "Вычислите, сколько токенов встречается в тексте **строго больше** ~~50~~ 10 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LO4yY-nFzYBs",
    "outputId": "8e358dc1-8eb7-4154-a7f5-08dd62fc251c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_more_than_10 = [token for token, count in freq_dist.items() if count > 10]\n",
    "num_tokens_more_than_10 = len(tokens_more_than_10)\n",
    "num_tokens_more_than_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6HZ2w3yxJEh"
   },
   "source": [
    "Проверьте себя: должно получиться значение ~~22~~ 276 (возможно небольшое расхождение)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
